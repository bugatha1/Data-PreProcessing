{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Preprocessing.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO7ZQ+/0dY2XbdOF4IkYvXh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bugatha1/Data-PreProcessing/blob/main/Preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Taking care of missing data"
      ],
      "metadata": {
        "id": "HDM3k6k2lyZi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# missing data filled with average/mean value\n",
        "\n",
        "from sklearn.impute import SimpleImputer\n",
        "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
        "imputer.fit(X[:, 1:3])\n",
        "X[:, 1:3] = imputer.transform(X[:, 1:3])"
      ],
      "metadata": {
        "id": "YW_3rawo9ujc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X)"
      ],
      "metadata": {
        "id": "kqH0sEAiCWvw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoding Categorical Data"
      ],
      "metadata": {
        "id": "G-RI6bwpEAnh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [0])], remainder='passthrough')\n",
        "X = np.array(ct.fit_transform(X))"
      ],
      "metadata": {
        "id": "-E0uhzHACfET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y)"
      ],
      "metadata": {
        "id": "DngNW0HrEL_D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Scaling"
      ],
      "metadata": {
        "id": "D4TVY3PiIFTQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature scaling is need to apply after dataset split into train and test.\n",
        "And feature scaling need to apply on only train set and not on test set\n",
        "\n",
        "In some scenarios, some feature dominating other features which is leading to information leakage. \n",
        "To avoid this we are using feature scaling\n",
        "\n",
        "Two main feature scaling techniques are 'standardisation' and 'normalisation' which will put all features in same scale\n",
        "\n",
        "Xstand = x - mean(x)/standard deviation(x)  (All the values will come between -3 and +3)\n",
        "\n",
        "xnorm = x- min(x)/max(x) - min(x)  ( all the values will come between 0 and 1)"
      ],
      "metadata": {
        "id": "33MS8fHps1DY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "\n",
        "X_train[:, 3:] = sc.fit_transform(X_train[:, 3:])\n",
        "X_test[:, 3:] = sc.transform(X_test[:, 3:])"
      ],
      "metadata": {
        "id": "A5QwbCOYtEtd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Steps in modelling with tensorflow\n",
        "\n",
        "1. Turn all data into numbers (neural networks can't handle strings)\n",
        "2. Make sure all of our tensors are in right shape\n",
        "3. Scale features(normalize or standardize, neural networks prefers normalization)"
      ],
      "metadata": {
        "id": "iOzN6Wx5l37U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Normalization** It is a technique often applied as part of data preparation for ML. The goal of normalization is to change the values of numeric columns in the dataset to a common scale, without distorting differences in the range of values.Every dataset doesn't require normalization."
      ],
      "metadata": {
        "id": "qOtj0IRKmg3L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "source : https://towardsdatascience.com/scale-standardize-or-normalize-with-scikit-learn-6ccc7d176a02  <br>\n",
        "\n",
        "normalization : converts all values to between 0 and 1 whilst preserving the original distribution<br>\n",
        "Scikit-learn function :  MinMaxScaler (this is a default scaler with neural networks)\n",
        "\n",
        "stnadardization: removes the mean and divides each value by the standard deviation.<br>\n",
        "Scikit-learn function :  StandardScaler (this reduces the effect of outliers)\n"
      ],
      "metadata": {
        "id": "OBeUt7az0ZwB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "w57jUpMFngqX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}