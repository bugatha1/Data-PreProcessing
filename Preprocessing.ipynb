{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Preprocessing.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOgWT504B8tlq+y2RDnsI7S",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bugatha1/Data-PreProcessing/blob/main/Preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Taking care of missing data"
      ],
      "metadata": {
        "id": "N7E-OjXD86ME"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# missing data filled with average/mean value\n",
        "\n",
        "from sklearn.impute import SimpleImputer\n",
        "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
        "imputer.fit(X[:, 1:3])\n",
        "X[:, 1:3] = imputer.transform(X[:, 1:3])"
      ],
      "metadata": {
        "id": "YW_3rawo9ujc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X)"
      ],
      "metadata": {
        "id": "kqH0sEAiCWvw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoding Categorical Data"
      ],
      "metadata": {
        "id": "G-RI6bwpEAnh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [0])], remainder='passthrough')\n",
        "X = np.array(ct.fit_transform(X))"
      ],
      "metadata": {
        "id": "-E0uhzHACfET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y)"
      ],
      "metadata": {
        "id": "DngNW0HrEL_D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Scaling"
      ],
      "metadata": {
        "id": "D4TVY3PiIFTQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature scaling is need to apply after dataset split into train and test.\n",
        "And feature scaling need to apply on only train set and not on test set\n",
        "\n",
        "In some scenarios, some feature dominating other features which is leading to information leakage. \n",
        "To avoid this we are using feature scaling\n",
        "\n",
        "Two main feature scaling techniques are 'standardisation' and 'normalisation' which will put all features in same scale\n",
        "\n",
        "Xstand = x - mean(x)/standard deviation(x)  (All the values will come between -3 and +3)\n",
        "\n",
        "xnorm = x- min(x)/max(x) - min(x)  ( all the values will come between 0 and 1)"
      ],
      "metadata": {
        "id": "33MS8fHps1DY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "\n",
        "X_train[:, 3:] = sc.fit_transform(X_train[:, 3:])\n",
        "X_test[:, 3:] = sc.transform(X_test[:, 3:])"
      ],
      "metadata": {
        "id": "A5QwbCOYtEtd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}